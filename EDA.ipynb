{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10aa05c1-9196-4a26-b525-76fa8a826fe4",
   "metadata": {},
   "source": [
    "# Project Goal\n",
    "\n",
    "The goal of this project is to **analyze customer ordering behavior and identify factors that contribute to cart abandonment** within the **MyCoke360** platform â€” Coca-Colaâ€™s online ordering system for business customers.\n",
    "\n",
    "MyCoke360 allows Coca-Colaâ€™s B2B clients â€” including restaurants, retailers, distributors, and institutions â€” to place and manage orders digitally.  \n",
    "Despite high engagement on the platform, many customers begin the purchase process but fail to complete their orders.\n",
    "\n",
    "This analysis aims to:\n",
    "- Quantify **cart abandonment rates**, and  \n",
    "- Uncover **behavioral and operational drivers** behind this issue.\n",
    "\n",
    "By linking **Google Analytics (GA)** event data (user interactions) with **order**, **visit**, and **sales** records, we seek to generate actionable insights that will help:\n",
    "- Reduce lost sales opportunities, and  \n",
    "- Improve the overall **conversion efficiency** of the MyCoke360 platform.\n",
    "\n",
    "---\n",
    "\n",
    "# Purpose of This EDA Notebook\n",
    "\n",
    "This **Exploratory Data Analysis (EDA)** notebook serves as the **data foundation** for the MyCoke360 Cart Abandonment Study.\n",
    "\n",
    "The objectives of this notebook are to:\n",
    "\n",
    "- **Explore and clean** all relevant datasets from MyCoke360â€™s data warehouse  \n",
    "- **Validate data quality** and ensure consistent linking across systems  \n",
    "- **Construct unified customer-level order windows** using visit frequency and cutoff times  \n",
    "- **Explore behavioral and operational trends** in ordering and cart activity  \n",
    "- **Identify potential drivers and hypotheses** that explain cart abandonment behavior  \n",
    "\n",
    "The outcomes of this EDA will inform future:\n",
    "- **Predictive modeling**,  \n",
    "- **Customer segmentation**, and  \n",
    "- **Strategic business recommendations**  \n",
    "\n",
    "to enhance MyCoke360â€™s digital customer experience and **increase order conversion rates**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b498c77-df9c-4bc3-96ac-37a5d58a1913",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "\n",
    "This project integrates multiple data sources from **Swire Coca-Colaâ€™s MyCoke360** platform to study **cart abandonment behavior** among business customers.  \n",
    "The dataset spans from **May 31, 2024, to May 26, 2025**, covering one full year of platform activity.\n",
    "\n",
    "Our analysis combines **behavioral**, **transactional**, and **operational** data to understand how customers interact with the MyCoke360 system and identify when they add products to their carts but fail to complete purchases.\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose of Data Integration\n",
    "\n",
    "Each dataset provides a different view of customer behavior and operations:\n",
    "\n",
    "| **Dataset** | **Description** | **Purpose in Analysis** |\n",
    "|--------------|----------------|-------------------------|\n",
    "| **Google Analytics (GA)** | Logs user interactions on MyCoke360 â€” including page views, button clicks, add/remove cart actions, and purchases. | Identifies **cart activity** and measures engagement and abandonment within order windows. |\n",
    "| **Orders** | Contains actual orders placed by customers, including timestamps, quantities, and materials. | Verifies whether a purchase occurred within the defined **order window** and corrects for missing GA purchase events. |\n",
    "| **Sales** | Captures financial details such as `NSI_DEAD_NET`, `PHYSICAL_VOLUME`, and `BOTTLER_PROFIT`. | Used to estimate **revenue loss** from abandoned carts and compute average order values. |\n",
    "| **Visit Plan History** | Historical schedule of each customerâ€™s delivery policy (anchor dates and frequency of orders). | Defines the expected **order windows** using anchor dates, frequencies, and cutoff times. |\n",
    "| **Operating Hours** | Current delivery frequency and anchor day for each customer. | Validates the latest delivery policy and helps link customers to visit plan data. |\n",
    "| **Cutoff Times** | Specifies order cutoff times for each plant and distribution mode. | Used to calculate **ANCHOR_CUTOFF** and **NEXT_ANCHOR_CUTOFF** per customer. |\n",
    "| **Customer** | Customer profile data including sales office, distribution mode, and cold drink channel. | Enables **segmentation** by region, delivery type, and customer category. |\n",
    "| **Material** | Product catalog with brand, pack type, flavor, and beverage category. | Helps identify **which product types** are more likely to be abandoned. |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Data Characteristics\n",
    "\n",
    "- **Date Range:** May 31, 2024 â€“ May 26, 2025  \n",
    "- **Customer Type:** B2B (Food Service On Premise) customers  \n",
    "- **Geographic Scope:** U.S. sales offices (e.g., Draper, Tempe, Denver, Wilsonville, Bellevue)  \n",
    "- **Unit of Analysis:** Customer Ã— Order Window  \n",
    "\n",
    "---\n",
    "\n",
    "## Data Volumes\n",
    "\n",
    "| **Dataset** | **File Name** | **Rows** | **Columns** |\n",
    "|--------------|---------------|-----------|--------------|\n",
    "| Google Analytics | `google_analytics.csv` | 3,704,088 | 10 |\n",
    "| Orders | `orders.csv` | 1,662,157 | 7 |\n",
    "| Sales | `sales.csv` | 499,787 | 8 |\n",
    "| Visit Plan History | `visit_plan.csv` | 14,796,017 | 9 |\n",
    "| Customer | `customer.csv` | 6,334 | 7 |\n",
    "| Cutoff Times | `cutoff_times.csv` | 220 | 5 |\n",
    "| Operating Hours | `operating_hours.csv` | 6,202 | 4 |\n",
    "| Material | `material.csv` | 1,252 | 6 |\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning and Preprocessing Steps\n",
    "\n",
    "The following preprocessing and validation steps were performed during EDA:\n",
    "\n",
    "### **Customer Table**\n",
    "- Dropped test or inactive accounts.  \n",
    "- Standardized column names (`CUSTOMER_NUMBER` â†’ `CUSTOMER_ID`).  \n",
    "- Cleaned `DISTRIBUTION_MODE_DESCRIPTION` (replaced nulls with \"OFS\").  \n",
    "\n",
    "### **Cutoff Times**\n",
    "- Dropped invalid rows with missing or zero-valued `SALES_OFFICE_DESCRIPTION`.  \n",
    "- Filled missing cutoff times with the default 5:00 PM value.  \n",
    "\n",
    "### **Visit Plan**\n",
    "- Removed nulls (<1% of records).  \n",
    "- Standardized frequency codes (`01` = weekly, `02` = biweekly, `04` = monthly).  \n",
    "- Validated `ANCHOR_DATE` and created calculated fields:  \n",
    "  ```python\n",
    "  ANCHOR_CUTOFF = ANCHOR_DATE + CUTOFF_TIME  \n",
    "  NEXT_ANCHOR_CUTOFF = ANCHOR_DATE + FREQUENCY + CUTOFF_TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3783699a-e82e-4f6d-8468-cb465db6b6c2",
   "metadata": {},
   "source": [
    "## Customer Table â€” Data Import and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad58a445-e84b-4e74-ae8f-bc79d64ff3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d244aa-5a17-4f08-8c4d-e489658f01ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/customer.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m customer \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/customer.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(customer\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*****\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/customer.csv'"
     ]
    }
   ],
   "source": [
    "customer = pd.read_csv(\"C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/customer.csv\")\n",
    "print(customer.head())\n",
    "print(\"*****\\n\")\n",
    "\n",
    "# Check for missing values in each column\n",
    "print(\"Missing values per column:\")\n",
    "print(customer.isnull().sum())\n",
    "print(\"*****\\n\")\n",
    "\n",
    "print(\"Value counts for 'DISTRIBUTION_MODE_DESCRIPTION':\")\n",
    "print(customer['DISTRIBUTION_MODE_DESCRIPTION'].value_counts(dropna=False))\n",
    "print(\"*****\\n\")\n",
    "\n",
    "customer_clean = customer.copy()\n",
    "# Fill missing values in 'DISTRIBUTION_MODE_DESCRIPTION' with 'OFS'\n",
    "customer_clean['DISTRIBUTION_MODE_DESCRIPTION'] = customer_clean['DISTRIBUTION_MODE_DESCRIPTION'].fillna('OFS')\n",
    "# Rename 'CUSTOMER_NUMBER' to 'CUSTOMER_ID' \n",
    "customer_clean = customer_clean.rename(columns={'CUSTOMER_NUMBER': 'CUSTOMER_ID'})\n",
    "# Display the first few rows of the cleaned dataset\n",
    "print(\"*****\\n\")\n",
    "print(\"First 5 rows of the cleaned customer dataset (customer_clean):\")\n",
    "print(customer_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229245fe-f001-4957-8e23-ca1afe13bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a graph that shows count of customers by distribution mode\n",
    "customer_clean.groupby('DISTRIBUTION_MODE_DESCRIPTION')['CUSTOMER_ID'].count()\\\n",
    ".sort_values(ascending=False)\\\n",
    ".plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba9d4c-6a23-4bf7-867d-2857b24858cc",
   "metadata": {},
   "source": [
    "## Cutoff Times Table â€” Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56acb47-3770-4655-b209-fc2d5cb4811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_times = pd.read_csv(\"C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/cutoff_times.csv\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in cutoff_times:\")\n",
    "print(cutoff_times.isnull().sum())\n",
    "print(\"*****\\n\")\n",
    "\n",
    "# Inspect unique values in DISTRIBUTION_MODE\n",
    "print(\"Distribution of 'DISTRIBUTION_MODE':\")\n",
    "print(cutoff_times['DISTRIBUTION_MODE'].value_counts(dropna=False))\n",
    "print(\"*****\\n\")\n",
    "\n",
    "# Rename columns for consistency with other tables\n",
    "cutoff_times = cutoff_times.rename(columns={\n",
    "    'DISTRIBUTION_MODE': 'DISTRIBUTION_MODE_DESC',\n",
    "    'SALES_OFFICE': 'SALES_OFFICE_DESCRIPTION'\n",
    "})\n",
    "\n",
    "# Check distribution of SALES_OFFICE_DESCRIPTION\n",
    "print(\"Value counts for 'SALES_OFFICE_DESCRIPTION':\")\n",
    "sales_office_counts = cutoff_times['SALES_OFFICE_DESCRIPTION'].value_counts()\n",
    "print(sales_office_counts)\n",
    "print(\"*****\\n\")\n",
    "\n",
    "# Remove 4 invalid rows where SALES_OFFICE_DESCRIPTION == '0'\n",
    "print(\"Removing rows where SALES_OFFICE_DESCRIPTION == '0' (invalid entries)...\")\n",
    "cutoff_clean = cutoff_times[cutoff_times['SALES_OFFICE_DESCRIPTION'] != '0']\n",
    "\n",
    "\n",
    "cutoff_clean.rename(columns = {'CUTOFFTIME__C':'CUTOFF_TIME'}, inplace= True)\n",
    "cutoff_clean[\"CUTOFF_TIME\"].fillna(\"5:00:00 PM\")\n",
    "print(cutoff_clean.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd326a00-7d1a-4c69-8f3b-c530567a2133",
   "metadata": {},
   "source": [
    "## Material Table â€” Data Import and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbdb197-a677-4534-a71b-99c015ac2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "material = pd.read_csv(\"C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/material.csv\")\n",
    "print(material.head())\n",
    "print(\"*****\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(material.isnull().sum())\n",
    "\n",
    "print(\"Value counts for 'BEV_CAT_DESC':\")\n",
    "print(material['BEV_CAT_DESC'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba7b89-ae0a-43e7-bd49-d200d6a61601",
   "metadata": {},
   "source": [
    "## Operating Hours Table â€” Data Import and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e61d89-7596-4b52-af5a-fcc811afedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "operating_hours = pd.read_csv(\"C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/operating_hours.csv\")\n",
    "print(operating_hours.head())\n",
    "print(\"*****\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "null_counts = operating_hours.isnull().sum()\n",
    "print(null_counts)\n",
    "\n",
    "# Inspect original 'FREQUENCY' values\n",
    "print(\"Original 'FREQUENCY' value counts:\")\n",
    "print(operating_hours['FREQUENCY'].value_counts(dropna=False))\n",
    "print(\"*****\\n\")\n",
    "# Map frequency descriptions to standardized codes\n",
    "map_frequency = {\n",
    "    'Every Week': '01',\n",
    "    'Every 2 Weeks': '02',\n",
    "    'Every 3 Weeks': '03',\n",
    "    'Every 4 Weeks': '04'\n",
    "}\n",
    "operating_hours['FREQUENCY_CLEAN'] = operating_hours['FREQUENCY'].map(map_frequency)\n",
    "print(operating_hours['FREQUENCY_CLEAN'].value_counts(dropna=False))\n",
    "print(\"\\nFrequency mapping complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c494ecb-6e19-47d2-acab-ff95bc2f9485",
   "metadata": {},
   "source": [
    "## Orders Table â€” Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b5e2b-76cd-48f5-85f0-1b7c9a3fdf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv(\"C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/orders.csv\")\n",
    "print(orders.head())\n",
    "print(\"******************\\n\")\n",
    "# Check for missing values\n",
    "null_counts = orders.isnull().sum()\n",
    "print(null_counts)\n",
    "\n",
    "print(f\"\\n Total rows: {len(orders):,}\")\n",
    "print(f\"Missing MATERIAL_ID: {null_counts['MATERIAL_ID']}, Missing PLANT_ID: {null_counts['PLANT_ID']}\")\n",
    "print(\"******************\\n\")\n",
    "\n",
    "# Remove rows with any missing values (especially critical for IDs)\n",
    "print(\"Dropping rows with missing IDs\")\n",
    "orders_clean = orders.dropna().copy()\n",
    "\n",
    "# Convert date columns to datetime\n",
    "print(\"Converting 'CREATED_DATE_EST' and 'CREATED_DATE_UTC' to datetime\")\n",
    "orders_clean['CREATED_DATE_EST'] = pd.to_datetime(orders_clean['CREATED_DATE_EST'])\n",
    "orders_clean['CREATED_DATE_UTC'] = pd.to_datetime(orders_clean['CREATED_DATE_UTC'])\n",
    "\n",
    "# Display date range for CREATED_DATE_UTC\n",
    "min_date = orders_clean['CREATED_DATE_UTC'].min()\n",
    "max_date = orders_clean['CREATED_DATE_UTC'].max()\n",
    "print(f\"ðŸ“… Date range (UTC): {min_date} to {max_date}\")\n",
    "print(\"******************\\n\")\n",
    "\n",
    "# Check for outliers in ORDER_QUANTITY\n",
    "print(\"Descriptive statistics for 'ORDER_QUANTITY':\")\n",
    "order_qty_stats = orders_clean['ORDER_QUANTITY'].describe()\n",
    "print(order_qty_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a7d292-7fbb-48dc-9372-76a67d88a1f4",
   "metadata": {},
   "source": [
    "## Sales Table â€” Data Import and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b16a18-72ca-468e-b6fb-798a9017eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\"C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/sales.csv\")\n",
    "print(sales.head())\n",
    "print(\"******************\\n\")\n",
    "# check for nulls\n",
    "print(sales.isnull().sum())\n",
    "# 5 highest values for nsi_dead_net\n",
    "print(sales.nlargest(5, 'NSI_DEAD_NET'))\n",
    "print(\"******************\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6afaf5-8e5a-4d06-8828-2bfbd354486a",
   "metadata": {},
   "source": [
    "## Visit Plan Table â€” Data Import, Cleaning, and Frequency Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13421e99-d45d-43a2-b85f-9234cb4bb072",
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_plan = pd.read_csv(\"C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/visit_plan.csv\")\n",
    "print(visit_plan.head())\n",
    "print(\"******************\\n\")\n",
    "\n",
    "# check for nulls\n",
    "print(\"Missing values per column:\")\n",
    "null_counts = visit_plan.isnull().sum()\n",
    "total_rows = len(visit_plan)\n",
    "print(null_counts)\n",
    "print(\"******************\\n\")\n",
    "\n",
    "# Map DISTRIBUTION_MODE codes to descriptions\n",
    "visit_plan['DISTRIBUTION_MODE'].value_counts()\n",
    "code_to_description = {\n",
    "    'OF': 'OFS',\n",
    "    'SL': 'Sideload',\n",
    "    'BK': 'Bulk Distribution',\n",
    "    'FS': 'Full Service',\n",
    "    'RD': 'Rapid Delivery',\n",
    "    'NR': 'Night Rapid Delivery',\n",
    "    'NS': 'Tell Sell',\n",
    "    'NO': 'Night OFS',\n",
    "    'EZ': 'E Pallet',\n",
    "    'SE': 'Special Events',\n",
    "    'DD': 'Night Sideload'\n",
    "}\n",
    "# creating distribution_mode_desc column\n",
    "visit_plan['DISTRIBUTION_MODE_DESC'] = visit_plan['DISTRIBUTION_MODE'].map(code_to_description)\n",
    "visit_plan['DISTRIBUTION_MODE_DESC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c857fb-19cc-4940-ba24-94dd8dac5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ANCHOR_DATE to datetime\n",
    "visit_clean = visit_plan.copy()\n",
    "visit_clean['ANCHOR_DATE'] = pd.to_datetime(visit_clean['ANCHOR_DATE'], errors='coerce')\n",
    "\n",
    "# Add day of week\n",
    "visit_clean['WEEK_DAY_OF_ANCHOR_DATE'] = visit_clean['ANCHOR_DATE'].dt.day_name()\n",
    "\n",
    "# Clean and standardize FREQUENCY column\n",
    "visit_clean['FREQUENCY'] = visit_clean['FREQUENCY'].astype(str).str.strip()\n",
    "\n",
    "# Apply mapping to standardize frequency values\n",
    "freq_mapping = {\n",
    "    \"01\": 1, \"1\": 1, \"1.0\": 1, \"1 \": 1, \" 1\": 1, \"Every Week On\": 1,\n",
    "    \"02\": 2, \"2\": 2, \"2.0\": 2, \"2 \": 2, \"Every Second Week On\": 2,\n",
    "    \"03\": 3, \"3\": 3, \"3.0\": 3, \"3 \": 3, \"Every Third Week On\": 3,\n",
    "    \"04\": 4, \"4\": 4, \"4.0\": 4, \"4 \": 4, \"Every Fourth Week On\": 4,\n",
    "    \"05\": 5, \"5\": 5, \"5.0\": 5, \"5 \": 5, \"Every Fifth Week On\": 5,\n",
    "    \"06\": 6, \"6\": 6, \"6.0\": 6, \"Every Sixth Week On\": 6,\n",
    "    \"08\": 8, \"8\": 8, \"8.0\": 8, \"Every Eighth Week On\": 8,\n",
    "    \"10\": 10, \"10.0\": 10, \"Every Tenth Week On\": 10,\n",
    "    \"Not Applicable\": None\n",
    "}\n",
    "\n",
    "# Map and convert to numeric\n",
    "visit_clean['FREQUENCY_NUM'] = visit_clean['FREQUENCY'].replace(freq_mapping)\n",
    "visit_clean['FREQUENCY_NUM'] = pd.to_numeric(visit_clean['FREQUENCY_NUM'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Convert to days (weeks * 7)\n",
    "visit_clean['WINDOW_FREQUENCY'] = visit_clean['FREQUENCY_NUM'] * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf9700-4dde-45d7-ab4e-f2227af574f3",
   "metadata": {},
   "source": [
    "## Google Analytics (GA) Data â€” Event-Level Cleaning and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb3b81-e682-4f73-94b9-4e4adec2e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = pd.read_csv(\"C:/Users/91744/Desktop/MSBA/sem 4/capstone/csv_files/google_analytics.csv\")\n",
    "print(ga.head())\n",
    "print(\"******************\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "null_counts = ga.isnull().sum()\n",
    "print(null_counts)\n",
    "\n",
    "# Show non-null counts for context\n",
    "print(\"Non-null counts per column:\")\n",
    "print(ga.count())\n",
    "print(\"******************\\n\")\n",
    "\n",
    "# Inspect EVENT_PAGE_TITLE vs EVENT_PAGE_NAME (often redundant)\n",
    "print(ga[['EVENT_PAGE_TITLE', 'EVENT_PAGE_NAME']].head())\n",
    "print(\"******************\\n\")\n",
    "\n",
    "ga_clean = ga.copy()\n",
    "\n",
    "# Convert EVENT_TIMESTAMP to datetime (UTC)\n",
    "print(\"Converting 'EVENT_TIMESTAMP' to datetime (UTC)...\")\n",
    "ga_clean[\"EVENT_TIMESTAMP_UTC\"] = (\n",
    "    pd.to_datetime(ga_clean[\"EVENT_TIMESTAMP\"], errors=\"coerce\")\n",
    "    .dt.tz_localize(None) \n",
    ")\n",
    "# Check how many timestamps failed conversion\n",
    "invalid_timestamps = ga_clean[\"EVENT_TIMESTAMP_UTC\"].isnull().sum()\n",
    "if invalid_timestamps > 0:\n",
    "    print(f\"{invalid_timestamps:,} rows have invalid timestamps (set to NaT).\")\n",
    "else:\n",
    "    print(\"All timestamps successfully parsed.\")\n",
    "print(\"******************\\n\")\n",
    "\n",
    "# Handle missing values in DEVICE_MOBILE_BRAND_NAME\n",
    "print(\"Top mobile brands (before cleaning):\")\n",
    "print(ga_clean['DEVICE_MOBILE_BRAND_NAME'].value_counts(dropna=False).head(10))\n",
    "print(\"******************\\n\")\n",
    "\n",
    "# Fill missing brand names with 'Unknown'\n",
    "ga_clean['DEVICE_MOBILE_BRAND_NAME'] = ga_clean['DEVICE_MOBILE_BRAND_NAME'].fillna('Unknown')\n",
    "print(ga_clean['DEVICE_MOBILE_BRAND_NAME'].value_counts().head(10))\n",
    "print(\"******************\\n\")\n",
    "\n",
    "# Drop redundant page columns\n",
    "print(\"Dropping redundant columns: 'EVENT_PAGE_TITLE', 'EVENT_PAGE_NAME'\")\n",
    "ga_clean = ga_clean.drop(columns=['EVENT_PAGE_TITLE', 'EVENT_PAGE_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e787295-d8ed-46c8-9546-82fddae5634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ece1ac-c6c4-4f38-9ade-edc0c8e7d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb5f76-1e4d-4a18-84d9-1a155c124571",
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_w_cutoff = pd.merge(\n",
    "    visit_clean,\n",
    "    cutoff_times,\n",
    "    left_on=[\"SALES_OFFICE\",\"SHIPPING_CONDITIONS_DESC\",\"DISTRIBUTION_MODE_DESC\"],\n",
    "    right_on=[\"PLANT_ID\",\"SHIPPING_CONDITION_TIME\",\"DISTRIBUTION_MODE_DESC\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "print(visit_w_cutoff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e823f93-43b4-40cb-9f1a-54509f26e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_w_cutoff = visit_w_cutoff.drop(columns=[\"FREQUENCY\", \"ELT_TS\",\"SNAPSHOT_DATE\",\"SALES_OFFICE_DESC\",\n",
    "                              \"SHIPPING_CONDITION_TIME\",\"PLANT_ID\"])\n",
    "print(visit_w_cutoff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc699c06-13b9-4eea-9ec4-aae56434be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_w_date = visit_w_cutoff.copy()\n",
    "# Master table included data out of our needed range\n",
    "visit_w_date['ANCHOR_DATE'] = pd.to_datetime(visit_w_date['ANCHOR_DATE'])\n",
    "visit_w_date = visit_w_date[(visit_w_date['ANCHOR_DATE'] > pd.to_datetime('2024-06-05')) & \n",
    "       (visit_w_date['ANCHOR_DATE'] < pd.to_datetime('2025-05-26'))]\n",
    "\n",
    "\n",
    "visit_w_date = visit_w_date.drop_duplicates(subset=['CUSTOMER_ID', 'ANCHOR_DATE']).sort_values('ANCHOR_DATE')\n",
    "print(visit_w_date.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38081faa-6777-43a4-bcf1-2406b3e27f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in between the unique ANCHOR_DATE with WINDOW_FREQUENCY\n",
    "\n",
    "order_window = visit_w_date.copy()\n",
    "\n",
    "order_window[\"ANCHOR_DATE\"] = pd.to_datetime(order_window[\"ANCHOR_DATE\"])\n",
    "order_window[\"WINDOW_FREQUENCY\"] = pd.to_numeric(order_window[\"WINDOW_FREQUENCY\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "wd_map = {'monday':0,'tuesday':1,'wednesday':2,'thursday':3,'friday':4,'saturday':5,'sunday':6}\n",
    "order_window[\"__wd\"] = order_window[\"WEEK_DAY_OF_ANCHOR_DATE\"].str.lower().map(wd_map)\n",
    "\n",
    "# end of each policy segment = next original ANCHOR_DATE (per customer), else +365d\n",
    "order_window = order_window.sort_values([\"CUSTOMER_ID\",\"ANCHOR_DATE\"]).reset_index(drop=True)\n",
    "order_window[\"__seg_end\"] = order_window.groupby(\"CUSTOMER_ID\")[\"ANCHOR_DATE\"].shift(-1)\n",
    "order_window[\"__seg_end\"] = order_window[\"__seg_end\"].fillna(order_window[\"ANCHOR_DATE\"] + pd.Timedelta(days=365))\n",
    "\n",
    "# first aligned NEXT for each original row\n",
    "cand = order_window[\"ANCHOR_DATE\"] + pd.to_timedelta(order_window[\"WINDOW_FREQUENCY\"].fillna(0), unit=\"D\")\n",
    "off  = (order_window[\"__wd\"] - cand.dt.weekday) % 7\n",
    "first_next = cand + pd.to_timedelta(off, unit=\"D\")\n",
    "\n",
    "# build ranges (use int(step) only when notna)\n",
    "ranges = [\n",
    "    pd.date_range(start=fn, end=se - pd.Timedelta(days=1), freq=f\"{int(step)}D\")\n",
    "    if (pd.notna(fn) and pd.notna(step) and fn < se) else pd.DatetimeIndex([])\n",
    "    for fn, se, step in zip(first_next, order_window[\"__seg_end\"], order_window[\"WINDOW_FREQUENCY\"])\n",
    "]\n",
    "\n",
    "# explode\n",
    "out = order_window.loc[order_window.index.repeat([len(r) for r in ranges])].copy()\n",
    "out[\"ANCHOR_DATE\"] = pd.DatetimeIndex([d for r in ranges for d in r])\n",
    "\n",
    "# compute the next aligned date for each expanded row\n",
    "next_cand = out[\"ANCHOR_DATE\"] + pd.to_timedelta(out[\"WINDOW_FREQUENCY\"], unit=\"D\")\n",
    "next_off  = (out[\"__wd\"] - next_cand.dt.weekday) % 7\n",
    "out[\"NEXT_ANCHOR_SAME_WD\"] = next_cand + pd.to_timedelta(next_off, unit=\"D\")\n",
    "\n",
    "# include original rows with their NEXT\n",
    "orig = order_window.copy()\n",
    "orig[\"NEXT_ANCHOR_SAME_WD\"] = first_next\n",
    "\n",
    "final = (\n",
    "    pd.concat([orig, out], ignore_index=True)\n",
    "      .drop(columns=[\"__wd\",\"__seg_end\"])\n",
    "      .sort_values([\"CUSTOMER_ID\",\"ANCHOR_DATE\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "order_window_final = final[(final['ANCHOR_DATE'] > pd.to_datetime('2024-06-05')) & \n",
    "       (final['ANCHOR_DATE'] < pd.to_datetime('2025-05-26'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e5390-dba7-4eb0-9cb1-99460a587de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same customer but now each rows are windows of order\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(order_window_final[order_window_final['CUSTOMER_ID']==501738077])\n",
    "len(order_window_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d648a1bd-a70d-42d9-91c6-04008da2218d",
   "metadata": {},
   "source": [
    "## Joining final table to orders dataÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a99eed-9469-4a01-ad0f-11cccfecfefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_size = 1000\n",
    "results = []\n",
    "\n",
    "# Sort orders once for efficient tail(1)\n",
    "orders_clean = orders_clean.sort_values(['CUSTOMER_ID', 'CREATED_DATE_EST'])\n",
    "\n",
    "for start in range(0, len(final), chunk_size):\n",
    "    end = start + chunk_size\n",
    "    order_window_chunk = order_window_final.iloc[start:end]\n",
    "\n",
    "    # Filter orders to only customers in this chunk\n",
    "    cust_ids = order_window_chunk['CUSTOMER_ID'].unique()\n",
    "    orders_subset = orders_clean[orders_clean['CUSTOMER_ID'].isin(cust_ids)]\n",
    "\n",
    "    # Merge and filter by anchor window\n",
    "    merged = pd.merge(order_window_chunk, orders_subset, on='CUSTOMER_ID', suffixes=('_final', '_order'))\n",
    "    merged = merged[\n",
    "        (merged['CREATED_DATE_EST'] >= merged['ANCHOR_DATE']) &\n",
    "        (merged['CREATED_DATE_EST'] < merged['NEXT_ANCHOR_SAME_WD'])\n",
    "    ]\n",
    "\n",
    "    # Get most recent order per anchor window\n",
    "    merged = merged.sort_values(['CUSTOMER_ID', 'ANCHOR_DATE', 'CREATED_DATE_EST'])\n",
    "    recent = merged.groupby(['CUSTOMER_ID', 'ANCHOR_DATE'], as_index=False).tail(1)\n",
    "\n",
    "    results.append(recent)\n",
    "\n",
    "# Combine all chunks\n",
    "recent_orders = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Merge back to final\n",
    "order_cols = ['CREATED_DATE_UTC', 'MATERIAL_ID', 'ORDER_QUANTITY', 'ORDER_TYPE', 'CREATED_DATE_EST']\n",
    "final_with_orders = pd.merge(\n",
    "    final,\n",
    "    recent_orders[['CUSTOMER_ID', 'ANCHOR_DATE'] + order_cols],\n",
    "    on=['CUSTOMER_ID', 'ANCHOR_DATE'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "final_with_orders['ORDER_EXISTS'] = final_with_orders['CREATED_DATE_EST'].notnull().astype(int)\n",
    "\n",
    "final_with_orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbed501-9e6f-4d30-bf9e-7a4aab711405",
   "metadata": {},
   "source": [
    "## Fixing timezones in GA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17e17e-eee9-412b-baef-ca5a392f180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Extract state from SALES_OFFICE_DESCRIPTION (format: \"City, ST\")\n",
    "final_with_orders[\"SALES_OFFICE_STATE\"] = (\n",
    "    final_with_orders['SALES_OFFICE_DESCRIPTION']\n",
    "    .str.split(',')\n",
    "    .str[1]\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Function for rough timezone offset (EST â†’ local)\n",
    "def est_to_local(event_time, state):\n",
    "    offsets = {\n",
    "        'AZ': -2,  # MST no DST from EST\n",
    "        'WA': -3, 'OR': -3, 'CA': -3,  # PDT\n",
    "        'CO': -2, 'NM': -2,  # MDT\n",
    "        'ID': -3,  # PDT approx\n",
    "        'NV': -3,  # PDT\n",
    "        'UT': -2,  # MDT\n",
    "        'NE': -2, 'WY': -2,  # MDT approx\n",
    "        # Add others as needed\n",
    "    }\n",
    "    offset_hours = offsets.get(state, -3)  # Default -3\n",
    "    return event_time + timedelta(hours=offset_hours)\n",
    "\n",
    "# âœ… Use final_with_orders since thatâ€™s where the column exists\n",
    "customer_state_map = (\n",
    "    final_with_orders[['CUSTOMER_ID', 'SALES_OFFICE_STATE']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('CUSTOMER_ID')['SALES_OFFICE_STATE']\n",
    ")\n",
    "\n",
    "# Map state to GA data\n",
    "ga_clean['SALES_OFFICE_STATE'] = ga_clean['CUSTOMER_ID'].map(customer_state_map)\n",
    "\n",
    "# Convert and apply time shift\n",
    "ga_clean['EVENT_TIMESTAMP'] = pd.to_datetime(ga_clean['EVENT_TIMESTAMP'])\n",
    "ga_clean['EVENT_TIMESTAMP_LOCAL'] = ga_clean.apply(\n",
    "    lambda row: est_to_local(row['EVENT_TIMESTAMP'], row['SALES_OFFICE_STATE']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(ga_clean.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8c2db-6aa9-414b-9b45-dfa08b42e7e7",
   "metadata": {},
   "source": [
    "## Joining with GA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50a56c-d9af-458f-8038-e1dd4588cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "ga_chunks = []\n",
    "\n",
    "# Sort GA events for consistency\n",
    "ga_events = ga_clean.sort_values(['CUSTOMER_ID', 'EVENT_DATE'])\n",
    "\n",
    "for start in range(0, len(final), chunk_size):\n",
    "    end = start + chunk_size\n",
    "    final_chunk = final_with_orders.iloc[start:end]\n",
    "\n",
    "    # Filter GA events to only customers in this chunk\n",
    "    cust_ids = final_chunk['CUSTOMER_ID'].unique()\n",
    "    ga_subset = ga_events[ga_events['CUSTOMER_ID'].isin(cust_ids)]\n",
    "\n",
    "    # Merge and filter by anchor window\n",
    "    merged = pd.merge(final_chunk, ga_subset, on='CUSTOMER_ID', suffixes=('_final', '_event'))\n",
    "    merged = merged[\n",
    "        (merged['EVENT_DATE'] >= merged['ANCHOR_DATE']) &\n",
    "        (merged['EVENT_DATE'] < merged['NEXT_ANCHOR_SAME_WD'])\n",
    "    ]\n",
    "\n",
    "    ga_chunks.append(merged)\n",
    "\n",
    "# Combine all matching events\n",
    "events_in_window = pd.concat(ga_chunks, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f939f7-3eef-4b74-bbaf-0fc91834d411",
   "metadata": {},
   "source": [
    "## Defining an Abandoned Cart and creating our final tableÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306151a-e4e0-4aa3-ad51-3d56cad420b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate GA data at cart level\n",
    "cart_level_df = (\n",
    "    events_in_window\n",
    "    .assign(in_window=lambda df:\n",
    "        (df[\"EVENT_DATE\"] >= df[\"ANCHOR_DATE\"]) &\n",
    "        (df[\"EVENT_DATE\"] < df[\"NEXT_ANCHOR_SAME_WD\"]))\n",
    "    .query(\"in_window\")\n",
    "    .groupby([\"CUSTOMER_ID\", \"ANCHOR_DATE\", \"NEXT_ANCHOR_SAME_WD\"], as_index=False)\n",
    "    .agg(\n",
    "        added_items=(\"EVENT_NAME\", lambda x: (x == \"add_to_cart\").sum()),\n",
    "        num_items_added=(\"ITEMS\", lambda x: sum(\n",
    "            len(items) for items in x[x.index[x.index.map(lambda i: events_in_window.loc[i, \"EVENT_NAME\"] == \"add_to_cart\")]]\n",
    "        )),\n",
    "        purchases=(\"EVENT_NAME\", lambda x: (x == \"purchase\").sum()),\n",
    "        button_clicks=(\"EVENT_NAME\", lambda x: (x == \"button_click\").sum()),\n",
    "        total_events=(\"EVENT_NAME\", \"count\")\n",
    "    )\n",
    "    .assign(\n",
    "        ABANDONED_CART=lambda df: ((df[\"added_items\"] > 0) & (df[\"purchases\"] == 0)).astype(int)\n",
    "    )\n",
    ")\n",
    "cart_level_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47712b96-9fc3-4790-a80e-ff585a6124fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to carts with added items\n",
    "cart_level_df = cart_level_df.query(\"added_items > 0\").copy()\n",
    "# abandoned cart rate\n",
    "cart_level_df['ABANDONED_CART'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd69fe-83fa-4438-879a-760c6e2a2220",
   "metadata": {},
   "source": [
    "Our abandoned Cart Rate is about 15.15%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6ccf7-68d9-406a-9ee9-4e642e245bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join back to final orders table\n",
    "final_with_ga = final_with_orders.merge(\n",
    "    cart_level_df,\n",
    "    on=[\"CUSTOMER_ID\", \"ANCHOR_DATE\", \"NEXT_ANCHOR_SAME_WD\"],\n",
    "    how=\"inner\"  # only keep rows where GA data exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6453c-bc2c-4e58-8be3-a0a4a059e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the final_orders_ga\n",
    "final_with_ga.to_csv('final_with_ga.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5122e0-e0c7-4d61-a2ce-d8994367904c",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281aace-b456-4ff7-815a-3f5cc75a4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "abandon = pd.read_csv(\"final_with_ga.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1ff58-71c8-468b-89ae-cf230095b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the abandon rate\n",
    "print(f\"The cart abandon rate is {abandon['ABANDONED_CART'].mean() * 100:.2f}%\")\n",
    "\n",
    "sns.countplot(x = abandon['ABANDONED_CART'])\n",
    "plt.title('The distribution of abandon cart and not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194a11e-974d-49be-acfc-86d50cc3596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which day of the week do customes tend to set their anchor date in\n",
    "sns.countplot(x='WEEK_DAY_OF_ANCHOR_DATE', data=abandon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ef483-c5f9-46ba-834e-e5b2a7bda644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the week customer usually purchase \n",
    "# Convert the 'CREATED_DATE_UTC' column to datetime if it isn't already\n",
    "abandon['CREATED_DATE_UTC'] = pd.to_datetime(abandon['CREATED_DATE_UTC'])\n",
    "\n",
    "# Extract day of the week (Monday=0, Sunday=6)\n",
    "abandon['Day_of_week_last_event'] = abandon['CREATED_DATE_UTC'].dt.day_name()\n",
    "\n",
    "# Plot the counts of each day\n",
    "ax = sns.countplot(x='Day_of_week_last_event', data=abandon[abandon['ABANDONED_CART']==1], order=[\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "plt.title('Count of Created Dates by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016bf037-d6cb-44cf-bcfe-a9ff39b54d69",
   "metadata": {},
   "source": [
    "Mondays and earlier weekdays seem to be the days when businesses tend to prep their stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffff81d-f778-416e-b56b-fe7f481dd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    x='SHIPPING_CONDITIONS_DESC', \n",
    "    y='ABANDONED_CART', \n",
    "    data=abandon, \n",
    "    estimator='mean', \n",
    "    errorbar=None, \n",
    "    palette='coolwarm'\n",
    ")\n",
    "plt.title('Abandonment Rate by Shipping Condition')\n",
    "plt.xlabel('Shipping Condition')\n",
    "plt.ylabel('Average Abandonment Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a1a14-8b51-4c4b-b71d-1c50d643cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,5))\n",
    "sns.barplot(\n",
    "    x='DISTRIBUTION_MODE_DESC', \n",
    "    y='ABANDONED_CART', \n",
    "    data=abandon, \n",
    "    estimator='mean', \n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Abandonment Rate by Distribution Mode')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average Abandonment Rate')\n",
    "plt.xlabel('Distribution Mode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc37c4-b7d8-4c56-b797-3a5d5d8e69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "abandon_customer = pd.merge(abandon, customer_clean, on='CUSTOMER_ID', how='left')\n",
    "\n",
    "sns.countplot(x='COLD_DRINK_CHANNEL_DESCRIPTION',hue='ABANDONED_CART', data=abandon_customer)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6056ea-36f7-4e41-afbf-adaaa7e55f2f",
   "metadata": {},
   "source": [
    "## Results & Findings\n",
    "\n",
    "This section summarizes the key patterns and insights derived from the exploratory data analysis conducted across multiple datasets â€” including orders, visits, customer profiles, sales, and Google Analytics. The goal was to identify behavioral and operational factors contributing to **cart abandonment**.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall Cart Abandonment Rate\n",
    "The overall **cart abandonment rate is approximately 15.1%**, indicating that a considerable proportion of customers initiate orders but do not complete them.  \n",
    "While a majority of users proceed to purchase, this 15% gap represents a strong opportunity area for optimization through better engagement and faster fulfillment.\n",
    "\n",
    "---\n",
    "\n",
    "### Day-of-Week Patterns\n",
    "- Customers tend to **set their anchor dates** and place orders more frequently on **Mondays and Wednesdays**.  \n",
    "- Abandoned carts are **least common on weekends**, suggesting that business buyers typically place and finalize orders early in the workweek when stock planning occurs.\n",
    "\n",
    "**Interpretation:**  \n",
    "Most activity aligns with B2B restocking cycles. Operational outreach or targeted reminders early in the week could help capture these high-intent windows.\n",
    "\n",
    "---\n",
    "\n",
    "### Cold Drink Channel Analysis\n",
    "- The **Restaurant** and **Distributor** segments generate the largest order volume, but also represent a significant share of total abandonment.  \n",
    "- The **Hot Beverage** channel shows the **highest abandonment percentage (around 27%)**, indicating irregular ordering or uncertain purchase intent.  \n",
    "- Channels like **Workplace** and **Clinic** maintain the lowest abandonment rates (~8â€“12%), reflecting more stable and predictable ordering behaviors.\n",
    "\n",
    "**Interpretation:**  \n",
    "Abandonment is most prevalent in categories where order timing is more flexible or optional (e.g., cafÃ©s and small distributors). Strengthening automated reordering or replenishment tools could improve conversion in these segments.\n",
    "\n",
    "---\n",
    "\n",
    "### Abandonment Rate by Shipping Condition\n",
    "- Customers facing **longer shipping windows (72 Hours)** show the **highest abandonment rate (~22%)**.  \n",
    "- **Faster delivery options (24 Hours and Dropsite 48 Hours)** have the **lowest abandonment rates (~10â€“15%)**.\n",
    "\n",
    "**Interpretation:**  \n",
    "Shipping speed plays a major role in purchase completion. Faster fulfillment likely boosts customer confidence and urgency to finalize orders.  \n",
    "Offering express options or clear delivery-time visibility could reduce abandonment in slower shipping categories.\n",
    "\n",
    "---\n",
    "\n",
    "### Abandonment Rate by Distribution Mode\n",
    "- The **â€˜Tell Sellâ€™** distribution mode shows the **highest abandonment (~30%)**, likely due to manual order handling and slower confirmation processes.  \n",
    "- More automated and efficient modes such as **Full Service**, **OFS**, and **Rapid Delivery** have much lower abandonment rates (~12â€“16%).  \n",
    "- **Bulk Distribution** also records moderate drop-offs, potentially due to larger, more complex orders.\n",
    "\n",
    "**Interpretation:**  \n",
    "Order fulfillment channels with greater automation and transparency tend to reduce customer hesitation.  \n",
    "Digitizing manual processes (e.g., â€˜Tell Sellâ€™) could yield immediate improvement in conversion rates.\n",
    "\n",
    "---\n",
    "\n",
    "### Abandonment by Visit Frequency\n",
    "- Customers with **lower visit frequency (monthly or infrequent buyers)** exhibit **higher abandonment**.  \n",
    "- Regular buyers, particularly those visiting weekly or biweekly, show **stronger follow-through rates**.\n",
    "\n",
    "**Interpretation:**  \n",
    "Engagement frequency strongly correlates with completion. Encouraging repeat visits through loyalty programs, personalized prompts, or scheduled order reminders could improve retention and reduce cart abandonment.\n",
    "\n",
    "---\n",
    "\n",
    "### Abandonment Rate by Sales Office\n",
    "- Offices such as **Richfield, UT**, **Price, UT**, and **Scottsbluff, NE** show the **highest abandonment rates (above 20%)**.  \n",
    "- Conversely, **Boise, ID**, **Ogden, UT**, and **Draper, UT** maintain **the lowest rates (below 10%)**, signaling stronger engagement and operational efficiency.\n",
    "\n",
    "**Interpretation:**  \n",
    "Regional disparities in performance may stem from differences in local order management, fulfillment reliability, or customer communication.  \n",
    "Prioritizing high-abandonment offices for sales training or process optimization could lead to significant improvement in overall conversion rates.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Key Insights\n",
    "\n",
    "| Factor | Observation | Business Implication |\n",
    "|--------|--------------|----------------------|\n",
    "| **Overall Abandonment** | 15.1% of carts are left incomplete | Clear opportunity for process and UX optimization |\n",
    "| **Day of Week** | Orders peak on Monday & Wednesday | Focus customer engagement early in the week |\n",
    "| **Shipping Condition** | Higher abandonment with longer delivery windows | Offer faster or transparent shipping options |\n",
    "| **Distribution Mode** | Manual â€œTell Sellâ€ has the highest abandonment | Automate or streamline manual order channels |\n",
    "| **Customer Frequency** | Regular visitors complete more purchases | Incentivize frequent ordering behavior |\n",
    "| **Regional Offices** | Large performance gaps between offices | Targeted interventions in high-abandonment regions |\n",
    "\n",
    "---\n",
    "\n",
    "**Overall Conclusion:**  \n",
    "Cart abandonment in this dataset appears to be driven by a mix of **logistical factors (delivery time, fulfillment type)** and **behavioral factors (visit frequency, timing)**.  \n",
    "By focusing on faster delivery, automation in manual processes, and improved engagement for infrequent buyers, the organization can meaningfully reduce its abandonment rate and improve sales efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8708b5a-2b17-4d46-ba94-0c14dcb4e2e1",
   "metadata": {},
   "source": [
    "## Group Member Contribution\n",
    "\n",
    "This project was completed as part of the **IS 6480: Capstone Project** course at the **University of Utah**, under the supervision of **Professor Jeff Webb** and **Swire Coca-Cola** as the industry partner.\n",
    "\n",
    "Our team collaborated closely throughout the project lifecycle â€” from data understanding and cleaning to modeling, analysis, and final presentation.  \n",
    "Below is a summary of each memberâ€™s key contributions:\n",
    "\n",
    "| **Team Member** | **Primary Contributions** |\n",
    "|------------------|---------------------------|\n",
    "| **Finlay Dunn** | Led exploratory data analysis (EDA), integrated Google Analytics (GA) event logs with Orders and Visit Plan datasets, defined the logic for order window construction, and calculated cart abandonment metrics.  |\n",
    "| **Huong** | Processed and merged the Visit Plan, and Cutoff Time datasets to create the master table. Contributed to data cleaning, frequency calculations, and initial GA event analysis. |\n",
    "| **Sudeeptha** | Focused on data validation and consistency checks across tables (Orders, Sales, GA). Compiled the notebook and assisted in visual summaries of order patterns and cart activity. |\n",
    "| **Shawal Fida** | Assisted in constructing descriptive statistics and visual summaries of order patterns and cart activity, contributed to data validation, and worked on interpreting the results and findings. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
